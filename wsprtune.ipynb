{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-09-15 update using code recommended by ChatGPT o1-mini when provided with the original code \n",
    "## and Gradio documentation for interface enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image  # Import PIL for image handling\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Device Configuration\n",
    "# ================================\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 2. Load and Prepare Data\n",
    "# ================================\n",
    "\n",
    "# Load training and testing data\n",
    "data_train = pd.read_csv('train.tsv', delimiter='\\t')\n",
    "data_test = pd.read_csv('test.tsv', delimiter='\\t')\n",
    "\n",
    "cv = DatasetDict()\n",
    "cv['train'] = Dataset.from_pandas(data_train)\n",
    "cv['test'] = Dataset.from_pandas(data_test)\n",
    "\n",
    "# Sanity check\n",
    "print(cv)\n",
    "\n",
    "# Set the sampling rate to 16kHz\n",
    "cv = cv.cast_column(\"path\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 3. Load Feature Extractor, Tokenizer, Processor\n",
    "# ================================\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small')\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', language='Russian', task='transcribe')\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-small', language='Russian', task='transcribe')\n",
    "\n",
    "# Sanity check for tokenizer\n",
    "input_str = cv['train'][0]['script']\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 4. Prepare the Data\n",
    "# ================================\n",
    "\n",
    "def prep_dataset(batch):\n",
    "    # Load and resample audio data to 16kHz\n",
    "    audio = batch['path']\n",
    "\n",
    "    # Compute log-mel spectrograms\n",
    "    batch['input_features'] = feature_extractor(audio['array'], sampling_rate=16000).input_features[0]\n",
    "   \n",
    "    # Encode target text to label ids\n",
    "    batch['labels'] = tokenizer(batch['script']).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the preprocessing to the datasets\n",
    "cv = cv.map(prep_dataset, remove_columns=cv['train'].column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 5. Define Data Collator\n",
    "# ================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # First treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # Pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If BOS token is appended in previous tokenization step, cut BOS token here as it's appended later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 6. Define Evaluation Metrics\n",
    "# ================================\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode the predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 7. Load and Fine-Tune the Whisper Model\n",
    "# ================================\n",
    "\n",
    "# Path to save or load the fine-tuned model\n",
    "fine_tuned_model_path = \"./wspr-smll-ru-04\"\n",
    "\n",
    "# Metrics storage\n",
    "fine_tuning_metrics = {}\n",
    "\n",
    "# Function to save metrics to a JSON file\n",
    "def save_metrics(metrics, path):\n",
    "    with open(os.path.join(path, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "# Function to load metrics from a JSON file\n",
    "def load_metrics(path):\n",
    "    with open(os.path.join(path, 'metrics.json'), 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Check if the fine-tuned model already exists\n",
    "if not os.path.exists(fine_tuned_model_path):\n",
    "    # Load the pretrained Whisper model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.to(device)\n",
    "\n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=fine_tuned_model_path,  # Directory to save the model\n",
    "        per_device_train_batch_size=8,     # Reduced batch size to save memory\n",
    "        gradient_accumulation_steps=2,     # To simulate larger batch size\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=50,\n",
    "        max_steps=500,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,                         # Use half-precision\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_eval_batch_size=4,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        save_steps=25,\n",
    "        eval_steps=25,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # Initialize the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=cv[\"train\"],\n",
    "        eval_dataset=cv[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    # Save the processor\n",
    "    processor.save_pretrained(fine_tuned_model_path)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Launch training\n",
    "    trainer.train()\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()    \n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model(fine_tuned_model_path)\n",
    "    \n",
    "    # Extract fine-tuning metrics\n",
    "    fine_tuning_metrics['fine_tuned_model_path'] = fine_tuned_model_path\n",
    "    fine_tuning_metrics['max_steps'] = training_args.max_steps\n",
    "    fine_tuning_metrics['batch_size'] = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "    fine_tuning_metrics['total_training_time'] = end_time - start_time  # in seconds\n",
    "\n",
    "    # Extract training loss from log_history\n",
    "    loss_history = []\n",
    "    eval_loss_history = []\n",
    "    eval_wer_history = []\n",
    "    for log in trainer.state.log_history:\n",
    "        if 'step' in log and 'loss' in log:\n",
    "            loss_history.append({'step': log['step'], 'loss': log['loss']})\n",
    "        if 'step' in log and 'eval_loss' in log:\n",
    "            eval_loss_history.append({'step': log['step'], 'eval_loss': log['eval_loss']})\n",
    "        if 'step' in log and 'eval_wer' in log:\n",
    "            eval_wer_history.append({'step': log['step'], 'eval_wer': log['eval_wer']})\n",
    "\n",
    "    # Store loss history for plotting\n",
    "    fine_tuning_metrics['train_loss_history'] = loss_history\n",
    "    fine_tuning_metrics['eval_loss_history'] = eval_loss_history\n",
    "    fine_tuning_metrics['eval_wer_history'] = eval_wer_history\n",
    "\n",
    "    # Save metrics to a JSON file\n",
    "    save_metrics(fine_tuning_metrics, fine_tuned_model_path)\n",
    "else:\n",
    "    print(f\"Fine-tuned model already exists at {fine_tuned_model_path}\")\n",
    "    metrics_file = os.path.join(fine_tuned_model_path, 'metrics.json')\n",
    "    if os.path.exists(metrics_file):\n",
    "        fine_tuning_metrics = load_metrics(fine_tuned_model_path)\n",
    "    else:\n",
    "        # Set placeholders if metrics are not available\n",
    "        fine_tuning_metrics['fine_tuned_model_path'] = fine_tuned_model_path\n",
    "        fine_tuning_metrics['max_steps'] = \"N/A (Metrics not available)\"\n",
    "        fine_tuning_metrics['batch_size'] = \"N/A (Metrics not available)\"\n",
    "        fine_tuning_metrics['total_training_time'] = \"N/A (Metrics not available)\"\n",
    "        fine_tuning_metrics['train_loss_history'] = []\n",
    "        fine_tuning_metrics['eval_loss_history'] = []\n",
    "        fine_tuning_metrics['eval_wer_history'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 8. Load Both Base and Fine-Tuned Models for Inference\n",
    "# ================================\n",
    "\n",
    "# Load the base Whisper model\n",
    "base_tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language='Russian', task='transcribe')\n",
    "base_pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    tokenizer=base_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# Load the fine-tuned Whisper model\n",
    "fine_tuned_tokenizer = WhisperTokenizer.from_pretrained(fine_tuned_model_path, language='Russian', task='transcribe')\n",
    "fine_tuned_pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=os.path.join(fine_tuned_model_path, \"checkpoint-250\"),  # Adjust checkpoint path as needed\n",
    "    tokenizer=fine_tuned_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 9. Define Inference Function\n",
    "# ================================\n",
    "\n",
    "def transcribe_both(audio):\n",
    "    \"\"\"\n",
    "    Transcribe the given audio file using both the base and fine-tuned Whisper models.\n",
    "\n",
    "    Args:\n",
    "        audio (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing transcriptions from both models.\n",
    "    \"\"\"\n",
    "    if audio is None:\n",
    "        return [\"No audio provided.\", \"No audio provided.\"]\n",
    "\n",
    "    try:\n",
    "        # Transcribe with base model\n",
    "        base_result = base_pipe(audio)\n",
    "        base_text = base_result.get(\"text\", \"Error in base model transcription.\")\n",
    "\n",
    "        # Transcribe with fine-tuned model\n",
    "        fine_tuned_result = fine_tuned_pipe(audio)\n",
    "        fine_tuned_text = fine_tuned_result.get(\"text\", \"Error in fine-tuned model transcription.\")\n",
    "\n",
    "        return [base_text, fine_tuned_text]\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during transcription: {str(e)}\"\n",
    "        return [error_message, error_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 10. Generate Training Loss Plot\n",
    "# ================================\n",
    "\n",
    "def generate_loss_plot(train_loss_history, eval_loss_history):\n",
    "    \"\"\"\n",
    "    Generates a matplotlib plot of training and evaluation loss over steps.\n",
    "\n",
    "    Args:\n",
    "        train_loss_history (list): List of dictionaries with 'step' and 'loss'.\n",
    "        eval_loss_history (list): List of dictionaries with 'step' and 'eval_loss'.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image or None: Image data in PIL format or None if no data.\n",
    "    \"\"\"\n",
    "    if not train_loss_history and not eval_loss_history:\n",
    "        return None  # No data to plot\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if train_loss_history:\n",
    "        steps = [entry['step'] for entry in train_loss_history]\n",
    "        losses = [entry['loss'] for entry in train_loss_history]\n",
    "        plt.plot(steps, losses, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "\n",
    "    if eval_loss_history:\n",
    "        eval_steps = [entry['step'] for entry in eval_loss_history]\n",
    "        eval_losses = [entry['eval_loss'] for entry in eval_loss_history]\n",
    "        plt.plot(eval_steps, eval_losses, marker='x', linestyle='--', color='r', label='Evaluation Loss')\n",
    "\n",
    "    plt.title('Training and Evaluation Loss over Steps')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to a bytes buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Open image with PIL and return\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "def generate_wer_plot(eval_wer_history):\n",
    "    \"\"\"\n",
    "    Generates a matplotlib plot of evaluation Word Error Rate (WER) over steps.\n",
    "\n",
    "    Args:\n",
    "        eval_wer_history (list): List of dictionaries with 'step' and 'eval_wer'.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image or None: Image data in PIL format or None if no data.\n",
    "    \"\"\"\n",
    "    if not eval_wer_history:\n",
    "        return None  # No data to plot\n",
    "\n",
    "    steps = [entry['step'] for entry in eval_wer_history]\n",
    "    wers = [entry['eval_wer'] for entry in eval_wer_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, wers, marker='s', linestyle='-', color='g', label='Evaluation WER (%)')\n",
    "    plt.title('Evaluation Word Error Rate (WER) over Steps')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('WER (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to a bytes buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Open image with PIL and return\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "# Generate the plots\n",
    "loss_plot = generate_loss_plot(\n",
    "    fine_tuning_metrics.get('train_loss_history', []),\n",
    "    fine_tuning_metrics.get('eval_loss_history', [])\n",
    ")\n",
    "\n",
    "wer_plot = generate_wer_plot(\n",
    "    fine_tuning_metrics.get('eval_wer_history', [])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 11. Create Gradio Interface\n",
    "# ================================\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Whisper ASR Comparison\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        Upload an audio file to see transcriptions from both the **Base Whisper Model** and the **Fine-Tuned Whisper Model**.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"Upload Audio\", type=\"filepath\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        transcribe_button = gr.Button(\"Transcribe\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Base Whisper Model Transcription\")\n",
    "            base_output = gr.Textbox(lines=10, interactive=False)\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Fine-Tuned Whisper Model Transcription\")\n",
    "            fine_tuned_output = gr.Textbox(lines=10, interactive=False)\n",
    "    \n",
    "    transcribe_button.click(\n",
    "        fn=transcribe_both,\n",
    "        inputs=audio_input,\n",
    "        outputs=[base_output, fine_tuned_output],\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    \n",
    "    gr.Markdown(\"## Fine-Tuning Metrics\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # Display Metrics without labels\n",
    "            gr.Markdown(\"**Fine-Tuned Model Path:**\")\n",
    "            model_path_display = gr.Textbox(value=fine_tuning_metrics.get('fine_tuned_model_path', 'N/A'), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Max Steps:**\")\n",
    "            max_steps_display = gr.Textbox(value=fine_tuning_metrics.get('max_steps', 'N/A'), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Batch Size:**\")\n",
    "            batch_size_display = gr.Textbox(value=fine_tuning_metrics.get('batch_size', 'N/A'), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Total Training Time (seconds):**\")\n",
    "            training_time_display = gr.Textbox(value=round(fine_tuning_metrics.get('total_training_time', 0), 2), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Latest Evaluation Loss:**\")\n",
    "            latest_eval_loss = \"N/A\"\n",
    "            if fine_tuning_metrics.get('eval_loss_history'):\n",
    "                latest_eval_loss = fine_tuning_metrics['eval_loss_history'][-1]['eval_loss']\n",
    "            gr.Textbox(value=latest_eval_loss, lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Latest Evaluation WER:**\")\n",
    "            latest_eval_wer = \"N/A\"\n",
    "            if fine_tuning_metrics.get('eval_wer_history'):\n",
    "                latest_eval_wer = fine_tuning_metrics['eval_wer_history'][-1]['eval_wer']\n",
    "            gr.Textbox(value=latest_eval_wer, lines=1, interactive=False)\n",
    "        with gr.Column(scale=1):\n",
    "            # Display Plots\n",
    "            gr.Markdown(\"### Training and Evaluation Loss Plot\")\n",
    "            if loss_plot:\n",
    "                loss_plot_display = gr.Image(\n",
    "                    label=None,\n",
    "                    value=loss_plot,\n",
    "                    interactive=False\n",
    "                )\n",
    "            else:\n",
    "                loss_plot_display = gr.Markdown(\"No loss data available.\")\n",
    "            \n",
    "            gr.Markdown(\"### Evaluation Word Error Rate (WER) Plot\")\n",
    "            if wer_plot:\n",
    "                wer_plot_display = gr.Image(\n",
    "                    label=None,\n",
    "                    value=wer_plot,\n",
    "                    interactive=False\n",
    "                )\n",
    "            else:\n",
    "                wer_plot_display = gr.Markdown(\"No WER data available.\")\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ---\n",
    "        Built with [Gradio](https://gradio.app) and [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "        \"\"\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
