{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-09-15 update using code recommended by ChatGPT o1-mini when provided with the original code \n",
    "## and Gradio documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image  # Import PIL for image handling\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 1. Device Configuration\n",
    "# ================================\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['path', 'script'],\n",
      "        num_rows: 52\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['path', 'script'],\n",
      "        num_rows: 12\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2. Load and Prepare Data\n",
    "# ================================\n",
    "\n",
    "# Load training and testing data\n",
    "data_train = pd.read_csv('train.tsv', delimiter='\\t')\n",
    "data_test = pd.read_csv('test.tsv', delimiter='\\t')\n",
    "\n",
    "cv = DatasetDict()\n",
    "cv['train'] = Dataset.from_pandas(data_train)\n",
    "cv['test'] = Dataset.from_pandas(data_test)\n",
    "\n",
    "# Sanity check\n",
    "print(cv)\n",
    "\n",
    "# Set the sampling rate to 16kHz\n",
    "cv = cv.cast_column(\"path\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 –°–ª—ã—à–∏–º, –ö—Ä—É–≥, —Ä–∞–∑—Ä–µ—à—ë–Ω —Å–ª–µ–¥—É–µ—Ç –ø—Ä—è–º–æ OSMIK. –ö—É—Ä—Å OSMIK, –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫-—Ç—Ä–∏-–≤–æ—Å–µ–º—å-–≤–æ—Å–µ–º—å, —Å–ø–∞—Å–∏–±–æ. \n",
      "Decoded w/ special:    <|startoftranscript|><|ru|><|transcribe|><|notimestamps|>–°–ª—ã—à–∏–º, –ö—Ä—É–≥, —Ä–∞–∑—Ä–µ—à—ë–Ω —Å–ª–µ–¥—É–µ—Ç –ø—Ä—è–º–æ OSMIK. –ö—É—Ä—Å OSMIK, –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫-—Ç—Ä–∏-–≤–æ—Å–µ–º—å-–≤–æ—Å–µ–º—å, —Å–ø–∞—Å–∏–±–æ. <|endoftext|>\n",
      "Decoded w/out special: –°–ª—ã—à–∏–º, –ö—Ä—É–≥, —Ä–∞–∑—Ä–µ—à—ë–Ω —Å–ª–µ–¥—É–µ—Ç –ø—Ä—è–º–æ OSMIK. –ö—É—Ä—Å OSMIK, –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫-—Ç—Ä–∏-–≤–æ—Å–µ–º—å-–≤–æ—Å–µ–º—å, —Å–ø–∞—Å–∏–±–æ. \n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 3. Load Feature Extractor, Tokenizer, Processor\n",
    "# ================================\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small')\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', language='Russian', task='transcribe')\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-small', language='Russian', task='transcribe')\n",
    "\n",
    "# Sanity check for tokenizer\n",
    "input_str = cv['train'][0]['script']\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/52 [00:00<?, ? examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):   2%|‚ñè         | 1/52 [00:00<00:17,  2.85 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  12%|‚ñà‚ñè        | 6/52 [00:00<00:03, 12.39 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  17%|‚ñà‚ñã        | 9/52 [00:00<00:03, 12.89 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  25%|‚ñà‚ñà‚ñå       | 13/52 [00:01<00:02, 13.68 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  35%|‚ñà‚ñà‚ñà‚ñç      | 18/52 [00:01<00:02, 14.74 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  38%|‚ñà‚ñà‚ñà‚ñä      | 20/52 [00:01<00:02, 13.42 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23/52 [00:01<00:01, 16.21 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25/52 [00:01<00:01, 13.94 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28/52 [00:02<00:01, 13.61 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31/52 [00:02<00:01, 16.22 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33/52 [00:02<00:01, 13.22 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36/52 [00:02<00:01, 13.03 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40/52 [00:02<00:00, 14.00 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42/52 [00:03<00:00, 12.67 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45/52 [00:03<00:00, 14.55 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47/52 [00:03<00:00, 13.13 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [00:03<00:00, 13.12 examples/s]\n",
      "Map (num_proc=4):   0%|          | 0/12 [00:00<?, ? examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):   8%|‚ñä         | 1/12 [00:00<00:03,  3.51 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [00:00<00:00, 10.68 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [00:00<00:00, 13.85 examples/s][src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:01<00:00, 11.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 4. Prepare the Data\n",
    "# ================================\n",
    "\n",
    "def prep_dataset(batch):\n",
    "    # Load and resample audio data to 16kHz\n",
    "    audio = batch['path']\n",
    "\n",
    "    # Compute log-mel spectrograms\n",
    "    batch['input_features'] = feature_extractor(audio['array'], sampling_rate=16000).input_features[0]\n",
    "   \n",
    "    # Encode target text to label ids\n",
    "    batch['labels'] = tokenizer(batch['script']).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the preprocessing to the datasets\n",
    "cv = cv.map(prep_dataset, remove_columns=cv['train'].column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 5. Define Data Collator\n",
    "# ================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # First treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # Pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If BOS token is appended in previous tokenization step, cut BOS token here as it's appended later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 6. Define Evaluation Metrics\n",
    "# ================================\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode the predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "  5%|‚ñå         | 25/500 [01:14<23:12,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5004, 'grad_norm': 8.717329978942871, 'learning_rate': 4.600000000000001e-06, 'epoch': 7.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "                                                \n",
      "  5%|‚ñå         | 25/500 [01:26<23:12,  2.93s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2244247198104858, 'eval_wer': 102.79720279720279, 'eval_runtime': 11.0648, 'eval_samples_per_second': 1.085, 'eval_steps_per_second': 0.271, 'epoch': 7.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 10%|‚ñà         | 50/500 [02:43<23:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4019, 'grad_norm': 2.3732190132141113, 'learning_rate': 9.600000000000001e-06, 'epoch': 14.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 10%|‚ñà         | 50/500 [02:56<23:00,  3.07s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8245256543159485, 'eval_wer': 100.0, 'eval_runtime': 12.7529, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.235, 'epoch': 14.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 15%|‚ñà‚ñå        | 75/500 [04:14<21:54,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0649, 'grad_norm': 1.8990089893341064, 'learning_rate': 9.48888888888889e-06, 'epoch': 21.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 15%|‚ñà‚ñå        | 75/500 [04:27<21:54,  3.09s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8177196979522705, 'eval_wer': 92.65734265734265, 'eval_runtime': 12.5577, 'eval_samples_per_second': 0.956, 'eval_steps_per_second': 0.239, 'epoch': 21.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 20%|‚ñà‚ñà        | 100/500 [05:45<20:40,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0145, 'grad_norm': 0.34284093976020813, 'learning_rate': 8.933333333333333e-06, 'epoch': 28.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 20%|‚ñà‚ñà        | 100/500 [05:57<20:40,  3.10s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8171883225440979, 'eval_wer': 77.27272727272727, 'eval_runtime': 11.7395, 'eval_samples_per_second': 1.022, 'eval_steps_per_second': 0.256, 'epoch': 28.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 25%|‚ñà‚ñà‚ñå       | 125/500 [07:15<19:49,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'grad_norm': 0.03412243351340294, 'learning_rate': 8.377777777777779e-06, 'epoch': 35.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 25%|‚ñà‚ñà‚ñå       | 125/500 [07:28<19:49,  3.17s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8253594040870667, 'eval_wer': 64.33566433566433, 'eval_runtime': 12.6022, 'eval_samples_per_second': 0.952, 'eval_steps_per_second': 0.238, 'epoch': 35.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 30%|‚ñà‚ñà‚ñà       | 150/500 [08:46<18:16,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'grad_norm': 0.015993455424904823, 'learning_rate': 7.822222222222224e-06, 'epoch': 42.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 30%|‚ñà‚ñà‚ñà       | 150/500 [08:59<18:16,  3.13s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8315367102622986, 'eval_wer': 65.38461538461539, 'eval_runtime': 12.4924, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.24, 'epoch': 42.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 175/500 [10:17<15:23,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'grad_norm': 0.011768111027777195, 'learning_rate': 7.266666666666668e-06, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 175/500 [10:29<15:23,  2.84s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.83879154920578, 'eval_wer': 81.46853146853147, 'eval_runtime': 12.7162, 'eval_samples_per_second': 0.944, 'eval_steps_per_second': 0.236, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/work/wspr/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 197/500 [11:39<15:35,  3.09s/it]"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 7. Load and Fine-Tune the Whisper Model\n",
    "# ================================\n",
    "\n",
    "# Path to save or load the fine-tuned model\n",
    "fine_tuned_model_path = \"./wspr-smll-ru-04\"\n",
    "\n",
    "# Metrics storage\n",
    "fine_tuning_metrics = {}\n",
    "\n",
    "# Function to save metrics to a JSON file\n",
    "def save_metrics(metrics, path):\n",
    "    with open(os.path.join(path, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "# Function to load metrics from a JSON file\n",
    "def load_metrics(path):\n",
    "    with open(os.path.join(path, 'metrics.json'), 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Check if the fine-tuned model already exists\n",
    "if not os.path.exists(fine_tuned_model_path):\n",
    "    # Load the pretrained Whisper model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.to(device)\n",
    "\n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=fine_tuned_model_path,  # Directory to save the model\n",
    "        per_device_train_batch_size=8,     # Reduced batch size to save memory\n",
    "        gradient_accumulation_steps=2,     # To simulate larger batch size\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=50,\n",
    "        max_steps=500,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,                         # Use half-precision\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_eval_batch_size=4,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        save_steps=25,\n",
    "        eval_steps=25,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # Initialize the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=cv[\"train\"],\n",
    "        eval_dataset=cv[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    # Save the processor\n",
    "    processor.save_pretrained(fine_tuned_model_path)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Launch training\n",
    "    trainer.train()\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()    \n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model(fine_tuned_model_path)\n",
    "    \n",
    "    # Extract fine-tuning metrics\n",
    "    fine_tuning_metrics['fine_tuned_model_path'] = fine_tuned_model_path\n",
    "    fine_tuning_metrics['max_steps'] = training_args.max_steps\n",
    "    fine_tuning_metrics['batch_size'] = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "    fine_tuning_metrics['total_training_time'] = end_time - start_time  # in seconds\n",
    "\n",
    "    # Extract training loss from log_history\n",
    "    loss_history = []\n",
    "    eval_loss_history = []\n",
    "    eval_wer_history = []\n",
    "    for log in trainer.state.log_history:\n",
    "        if 'step' in log and 'loss' in log:\n",
    "            loss_history.append({'step': log['step'], 'loss': log['loss']})\n",
    "        if 'step' in log and 'eval_loss' in log:\n",
    "            eval_loss_history.append({'step': log['step'], 'eval_loss': log['eval_loss']})\n",
    "        if 'step' in log and 'eval_wer' in log:\n",
    "            eval_wer_history.append({'step': log['step'], 'eval_wer': log['eval_wer']})\n",
    "\n",
    "    # Store loss history for plotting\n",
    "    fine_tuning_metrics['train_loss_history'] = loss_history\n",
    "    fine_tuning_metrics['eval_loss_history'] = eval_loss_history\n",
    "    fine_tuning_metrics['eval_wer_history'] = eval_wer_history\n",
    "\n",
    "    # Save metrics to a JSON file\n",
    "    save_metrics(fine_tuning_metrics, fine_tuned_model_path)\n",
    "else:\n",
    "    print(f\"Fine-tuned model already exists at {fine_tuned_model_path}\")\n",
    "    metrics_file = os.path.join(fine_tuned_model_path, 'metrics.json')\n",
    "    if os.path.exists(metrics_file):\n",
    "        fine_tuning_metrics = load_metrics(fine_tuned_model_path)\n",
    "    else:\n",
    "        # Set placeholders if metrics are not available\n",
    "        fine_tuning_metrics['fine_tuned_model_path'] = fine_tuned_model_path\n",
    "        fine_tuning_metrics['max_steps'] = \"N/A (Metrics not available)\"\n",
    "        fine_tuning_metrics['batch_size'] = \"N/A (Metrics not available)\"\n",
    "        fine_tuning_metrics['total_training_time'] = \"N/A (Metrics not available)\"\n",
    "        fine_tuning_metrics['train_loss_history'] = []\n",
    "        fine_tuning_metrics['eval_loss_history'] = []\n",
    "        fine_tuning_metrics['eval_wer_history'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 8. Load Both Base and Fine-Tuned Models for Inference\n",
    "# ================================\n",
    "\n",
    "# Load the base Whisper model\n",
    "base_tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language='Russian', task='transcribe')\n",
    "base_pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    tokenizer=base_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n",
    "\n",
    "# Load the fine-tuned Whisper model\n",
    "fine_tuned_tokenizer = WhisperTokenizer.from_pretrained(fine_tuned_model_path, language='Russian', task='transcribe')\n",
    "fine_tuned_pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=os.path.join(fine_tuned_model_path, \"checkpoint-250\"),  # Adjust checkpoint path as needed\n",
    "    tokenizer=fine_tuned_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 9. Define Inference Function\n",
    "# ================================\n",
    "\n",
    "def transcribe_both(audio):\n",
    "    \"\"\"\n",
    "    Transcribe the given audio file using both the base and fine-tuned Whisper models.\n",
    "\n",
    "    Args:\n",
    "        audio (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing transcriptions from both models.\n",
    "    \"\"\"\n",
    "    if audio is None:\n",
    "        return [\"No audio provided.\", \"No audio provided.\"]\n",
    "\n",
    "    try:\n",
    "        # Transcribe with base model\n",
    "        base_result = base_pipe(audio)\n",
    "        base_text = base_result.get(\"text\", \"Error in base model transcription.\")\n",
    "\n",
    "        # Transcribe with fine-tuned model\n",
    "        fine_tuned_result = fine_tuned_pipe(audio)\n",
    "        fine_tuned_text = fine_tuned_result.get(\"text\", \"Error in fine-tuned model transcription.\")\n",
    "\n",
    "        return [base_text, fine_tuned_text]\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during transcription: {str(e)}\"\n",
    "        return [error_message, error_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 10. Generate Training Loss Plot\n",
    "# ================================\n",
    "\n",
    "def generate_loss_plot(train_loss_history, eval_loss_history):\n",
    "    \"\"\"\n",
    "    Generates a matplotlib plot of training and evaluation loss over steps.\n",
    "\n",
    "    Args:\n",
    "        train_loss_history (list): List of dictionaries with 'step' and 'loss'.\n",
    "        eval_loss_history (list): List of dictionaries with 'step' and 'eval_loss'.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image or None: Image data in PIL format or None if no data.\n",
    "    \"\"\"\n",
    "    if not train_loss_history and not eval_loss_history:\n",
    "        return None  # No data to plot\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if train_loss_history:\n",
    "        steps = [entry['step'] for entry in train_loss_history]\n",
    "        losses = [entry['loss'] for entry in train_loss_history]\n",
    "        plt.plot(steps, losses, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "\n",
    "    if eval_loss_history:\n",
    "        eval_steps = [entry['step'] for entry in eval_loss_history]\n",
    "        eval_losses = [entry['eval_loss'] for entry in eval_loss_history]\n",
    "        plt.plot(eval_steps, eval_losses, marker='x', linestyle='--', color='r', label='Evaluation Loss')\n",
    "\n",
    "    plt.title('Training and Evaluation Loss over Steps')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to a bytes buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Open image with PIL and return\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "def generate_wer_plot(eval_wer_history):\n",
    "    \"\"\"\n",
    "    Generates a matplotlib plot of evaluation Word Error Rate (WER) over steps.\n",
    "\n",
    "    Args:\n",
    "        eval_wer_history (list): List of dictionaries with 'step' and 'eval_wer'.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image or None: Image data in PIL format or None if no data.\n",
    "    \"\"\"\n",
    "    if not eval_wer_history:\n",
    "        return None  # No data to plot\n",
    "\n",
    "    steps = [entry['step'] for entry in eval_wer_history]\n",
    "    wers = [entry['eval_wer'] for entry in eval_wer_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, wers, marker='s', linestyle='-', color='g', label='Evaluation WER (%)')\n",
    "    plt.title('Evaluation Word Error Rate (WER) over Steps')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('WER (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to a bytes buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Open image with PIL and return\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "# Generate the plots\n",
    "loss_plot = generate_loss_plot(\n",
    "    fine_tuning_metrics.get('train_loss_history', []),\n",
    "    fine_tuning_metrics.get('eval_loss_history', [])\n",
    ")\n",
    "\n",
    "wer_plot = generate_wer_plot(\n",
    "    fine_tuning_metrics.get('eval_wer_history', [])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 11. Create Gradio Interface\n",
    "# ================================\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Whisper ASR Comparison\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        Upload an audio file to see transcriptions from both the **Base Whisper Model** and the **Fine-Tuned Whisper Model**.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"Upload Audio\", type=\"filepath\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        transcribe_button = gr.Button(\"Transcribe\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Base Whisper Model Transcription\")\n",
    "            base_output = gr.Textbox(lines=10, interactive=False)\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Fine-Tuned Whisper Model Transcription\")\n",
    "            fine_tuned_output = gr.Textbox(lines=10, interactive=False)\n",
    "    \n",
    "    transcribe_button.click(\n",
    "        fn=transcribe_both,\n",
    "        inputs=audio_input,\n",
    "        outputs=[base_output, fine_tuned_output],\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    \n",
    "    gr.Markdown(\"## Fine-Tuning Metrics\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # Display Metrics without labels\n",
    "            gr.Markdown(\"**Fine-Tuned Model Path:**\")\n",
    "            model_path_display = gr.Textbox(value=fine_tuning_metrics.get('fine_tuned_model_path', 'N/A'), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Max Steps:**\")\n",
    "            max_steps_display = gr.Textbox(value=fine_tuning_metrics.get('max_steps', 'N/A'), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Batch Size:**\")\n",
    "            batch_size_display = gr.Textbox(value=fine_tuning_metrics.get('batch_size', 'N/A'), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Total Training Time (seconds):**\")\n",
    "            training_time_display = gr.Textbox(value=round(fine_tuning_metrics.get('total_training_time', 0), 2), lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Latest Evaluation Loss:**\")\n",
    "            latest_eval_loss = \"N/A\"\n",
    "            if fine_tuning_metrics.get('eval_loss_history'):\n",
    "                latest_eval_loss = fine_tuning_metrics['eval_loss_history'][-1]['eval_loss']\n",
    "            gr.Textbox(value=latest_eval_loss, lines=1, interactive=False)\n",
    "            \n",
    "            gr.Markdown(\"**Latest Evaluation WER:**\")\n",
    "            latest_eval_wer = \"N/A\"\n",
    "            if fine_tuning_metrics.get('eval_wer_history'):\n",
    "                latest_eval_wer = fine_tuning_metrics['eval_wer_history'][-1]['eval_wer']\n",
    "            gr.Textbox(value=latest_eval_wer, lines=1, interactive=False)\n",
    "        with gr.Column(scale=1):\n",
    "            # Display Plots\n",
    "            gr.Markdown(\"### Training and Evaluation Loss Plot\")\n",
    "            if loss_plot:\n",
    "                loss_plot_display = gr.Image(\n",
    "                    label=None,\n",
    "                    value=loss_plot,\n",
    "                    interactive=False\n",
    "                )\n",
    "            else:\n",
    "                loss_plot_display = gr.Markdown(\"No loss data available.\")\n",
    "            \n",
    "            gr.Markdown(\"### Evaluation Word Error Rate (WER) Plot\")\n",
    "            if wer_plot:\n",
    "                wer_plot_display = gr.Image(\n",
    "                    label=None,\n",
    "                    value=wer_plot,\n",
    "                    interactive=False\n",
    "                )\n",
    "            else:\n",
    "                wer_plot_display = gr.Markdown(\"No WER data available.\")\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ---\n",
    "        Built with [Gradio](https://gradio.app) and [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "        \"\"\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
